import numpy as np

def gradient(X, Y, initial_learning_rate=0.01, decay_rate=0.01, n_iterations=1000):
    m = len(Y)
    theta = np.random.randn(X.shape[1])  
    for iteration in range(n_iterations):
        predictions = X.dot(theta)
        errors = predictions - Y
        gradients = (2/m) * X.T.dot(errors)
        learning_rate = initial_learning_rate / (1 + decay_rate * iteration)
        theta -= learning_rate * gradients
        
        error = np.mean(errors**2)
        
        if error < 0.1:
            break

    return theta



theta_gd = gradient(X, Y)
print("\nGradient descent result (theta):", theta_gd)

Output:-
Gradient descent result (theta): [ 5.18519546e+21 -2.96544379e+20]
